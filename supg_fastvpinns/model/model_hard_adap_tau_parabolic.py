# A method which hosts the NN model and the training loop for variational Pinns with HARD CONSTRAINTS
# This focusses only on the model architecture and the training loop, and not on the loss functions
# Author : Thivin Anandh D
# Date : 22/Sep/2023
# History : 22/Sep/2023 - Initial implementation with basic model architecture and training loop

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import initializers
import copy
import math
# tf.config.run_functions_eagerly(True)

# Custom Loss Functions
def custom_loss1(y_true1, y_pred1):
    return tf.reduce_mean(tf.square(y_pred1 - y_true1))

def custom_loss2(y_true2, y_pred2):
    return tf.reduce_mean(tf.square(y_pred2 - y_true2))

# Custom Model
class DenseModel(tf.keras.Model):
    """
    This class defines the Dense Model for the Neural Network for solving Variational PINNs

    Attributes:
    - layer_dims (list): List of dimensions of the dense layers
    - use_attention (bool): Flag to use attention layer after input
    - activation (str): The activation function to be used for the dense layers
    - layer_list (list): List of dense layers
    - loss_function (function): The loss function for the PDE
    - hessian (bool): Flag to use hessian loss
    - input_tensor (tf.Tensor): The input tensor for the PDE
    - dirichlet_input (tf.Tensor): The input tensor for the Dirichlet boundary data
    - dirichlet_actual (tf.Tensor): The actual values for the Dirichlet boundary data
    - optimizer (tf.keras.optimizers): The optimizer for the model
    - gradients (tf.Tensor): The gradients of the loss function wrt the trainable variables
    - learning_rate_dict (dict): The dictionary containing the learning rate parameters
    - orig_factor_matrices (list): The list containing the original factor matrices
    - shape_function_mat_list (tf.Tensor): The shape function matrix
    - shape_function_grad_x_factor_mat_list (tf.Tensor): The shape function derivative with respect to x matrix
    - shape_function_grad_y_factor_mat_list (tf.Tensor): The shape function derivative with respect to y matrix
    - force_function_list (tf.Tensor): The force function matrix
    - input_tensors_list (list): The list containing the input tensors
    - params_dict (dict): The dictionary containing the parameters
    - pre_multiplier_val (tf.Tensor): The pre-multiplier for the shape function matrix
    - pre_multiplier_grad_x (tf.Tensor): The pre-multiplier for the shape function derivative with respect to x matrix
    - pre_multiplier_grad_y (tf.Tensor): The pre-multiplier for the shape function derivative with respect to y matrix
    - force_matrix (tf.Tensor): The force function matrix
    - n_cells (int): The number of cells in the domain
    - tensor_dtype (tf.DType): The tensorflow dtype to be used for all the tensors

    Methods:
    - call(inputs): The call method for the model
    - get_config(): Returns the configuration of the model
    - train_step(beta, bilinear_params_dict): The train step method for the model
    """
    def __init__(self, layer_dims, learning_rate_dict, params_dict, loss_function, input_tensors_list, orig_factor_matrices , force_function_list, \
                 real_forcing_function, \
                 tensor_dtype, \
                 use_attention=False, activation='tanh', hessian=False):
        super(DenseModel, self).__init__()
        self.layer_dims = layer_dims
        self.use_attention = use_attention
        self.activation = activation
        self.layer_list = []
        self.loss_function = loss_function
        self.hessian = hessian
        self.real_forcing_function = real_forcing_function

        self.tensor_dtype = tensor_dtype

        # if dtype is not a valid tensorflow dtype, raise an error
        if not isinstance(self.tensor_dtype, tf.DType):
            raise TypeError("The given dtype is not a valid tensorflow dtype")

        self.orig_factor_matrices = orig_factor_matrices
        self.shape_function_mat_list = copy.deepcopy(orig_factor_matrices[0])
        self.shape_function_grad_x_factor_mat_list = copy.deepcopy(orig_factor_matrices[1])
        self.shape_function_grad_y_factor_mat_list = copy.deepcopy(orig_factor_matrices[2])
        
        self.force_function_list = force_function_list

        self.input_tensors_list = input_tensors_list
        self.input_tensor = copy.deepcopy(input_tensors_list[0])
        self.dirichlet_input = copy.deepcopy(input_tensors_list[1])
        self.dirichlet_actual = copy.deepcopy(input_tensors_list[2])
        
        self.k1 = tf.constant(30, dtype = tf.float64)
        self.k = tf.constant(10/1e-8, dtype = tf.float64)
        self.params_dict = params_dict

        self.pre_multiplier_val   = self.shape_function_mat_list
        self.pre_multiplier_grad_x = self.shape_function_grad_x_factor_mat_list
        self.pre_multiplier_grad_y = self.shape_function_grad_y_factor_mat_list

        self.force_matrix = self.force_function_list
        
        print(f"{'-'*74}")
        print(f"| {'PARAMETER':<25} | {'SHAPE':<25} |")
        print(f"{'-'*74}")
        print(f"| {'input_tensor':<25} | {str(self.input_tensor.shape):<25} | {self.input_tensor.dtype}")
        print(f"| {'force_matrix':<25} | {str(self.force_matrix.shape):<25} | {self.force_matrix.dtype}")
        print(f"| {'pre_multiplier_grad_x':<25} | {str(self.pre_multiplier_grad_x.shape):<25} | {self.pre_multiplier_grad_x.dtype}")
        print(f"| {'pre_multiplier_grad_y':<25} | {str(self.pre_multiplier_grad_y.shape):<25} | {self.pre_multiplier_grad_y.dtype}")
        print(f"| {'pre_multiplier_val':<25} | {str(self.pre_multiplier_val.shape):<25} | {self.pre_multiplier_val.dtype}")
        print(f"| {'dirichlet_input':<25} | {str(self.dirichlet_input.shape):<25} | {self.dirichlet_input.dtype}")
        print(f"| {'dirichlet_actual':<25} | {str(self.dirichlet_actual.shape):<25} | {self.dirichlet_actual.dtype}")
        print(f"{'-'*74}")
        
        self.n_cells = params_dict['n_cells']

        # self.attention_layer = AttentionLayer(self.n_cells)

        ## ----------------------------------------------------------------- ##
        ## ---------- LEARNING RATE AND OPTIMISER FOR THE MODEL ------------ ##
        ## ----------------------------------------------------------------- ##

        # parse the learning rate dictionary
        self.learning_rate_dict = learning_rate_dict
        initial_learning_rate = learning_rate_dict['initial_learning_rate']
        use_lr_scheduler = learning_rate_dict['use_lr_scheduler']
        decay_steps = learning_rate_dict['decay_steps']
        decay_rate = learning_rate_dict['decay_rate']
        staircase = learning_rate_dict['staircase']

        if(use_lr_scheduler):
            learning_rate_fn = tf.keras.optimizers.schedules.ExponentialDecay(
                initial_learning_rate, decay_steps, decay_rate, staircase=True
            )
        else:
            learning_rate_fn = initial_learning_rate

        
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)


        ## ----------------------------------------------------------------- ##
        ## --------------------- MODEL ARCHITECTURE ------------------------ ##
        ## ----------------------------------------------------------------- ##
        
        # Build dense layers based on the input list
        for dim in range(len(self.layer_dims) - 2):
            self.layer_list.append(layers.Dense(self.layer_dims[dim+1], activation=self.activation, \
                                                    kernel_initializer='glorot_uniform', \
                                                    dtype=self.tensor_dtype, bias_initializer='zeros'))
        
        # Add a output layer with no activation
        self.layer_list.append(layers.Dense(self.layer_dims[-1], activation=None, 
                                    kernel_initializer='glorot_uniform',
                                    dtype=self.tensor_dtype, bias_initializer='zeros'))
        

        # Add attention layer if required
        if self.use_attention:
            self.attention_layer = layers.Attention()
        
        # Compile the model
        self.compile(optimizer=self.optimizer)
        self.build(input_shape=(None, self.layer_dims[0]))
        
        # print the summary of the model
        self.summary()

        
    def call(self, inputs):
        x = inputs

        # Apply attention layer after input if flag is True
        if self.use_attention:
            x = self.attention_layer([x, x])

        # Loop through the dense layers
        for layer in self.layer_list:
            x = layer(x)

        return x
    
    def get_config(self):
        # Get the base configuration
        base_config = super().get_config()

        # Add the non-serializable arguments to the configuration
        base_config.update({
            'learning_rate_dict': self.learning_rate_dict,
            'loss_function': self.loss_function,
            'input_tensors_list':  self.input_tensors_list,
            'orig_factor_matrices': self.orig_factor_matrices,
            'force_function_list': self.force_function_list,
            'params_dict': self.params_dict,
            'use_attention': self.use_attention,
            'activation': self.activation,
            'hessian': self.hessian,
            'layer_dims': self.layer_dims,
            'tensor_dtype': self.tensor_dtype
        })

        return base_config
    


    @tf.function
    def train_step(self, beta=10, bilinear_params_dict=None, l2_lambda = 0.0001, pd_lambda = 0.0001, tau = 0.01):

        with tf.GradientTape(persistent=True) as tape:
            # Predict the values for dirichlet boundary conditions
            predicted_values_dirichlet = self(self.dirichlet_input)[:,0:1]

            # initialize total loss as a tensor with shape (1,) and value 0.0
            total_pde_loss = 0.0
            with tf.GradientTape(persistent=True) as tape2:
            # With the inner tape, watch the input tensor
                tape2.watch(self.input_tensor)
                with tf.GradientTape(persistent=True) as tape1:
                    # tape gradient
                    tape1.watch(self.input_tensor)
                    # Compute the predicted values from the model
                    # aa = tf.math.sin(tf.cast(tf.constant(math.pi),tf.float64)*self.input_tensor[:, 0:1]) * tf.math.sin(tf.cast(tf.constant(math.pi),tf.float64)*self.input_tensor[:, 1:2])
                    k1 = 30
                    k = 10/1e-8
                    bb = tf.tanh(50*self.input_tensor[:, 0:1]) * tf.tanh(50*self.input_tensor[:, 1:2]) * tf.tanh(50*(1-self.input_tensor[:, 0:1])) *tf.tanh(50*(1-self.input_tensor[:, 1:2]))
                    
                    #For outflow layer
                    aa = (1 - tf.exp(-k1*self.input_tensor[:, 0:1])) * (1 - tf.exp(-k*self.input_tensor[:, 1:2])) \
                        * (1 - tf.exp(-k * (1 - self.input_tensor[:, 0:1]))) * (1 - tf.exp(-k * (1 - self.input_tensor[:, 1:2])))                    
                    
                    aa = tf.cast(aa, tf.float64)
                    bb = tf.cast(bb, tf.float64)
                    predicted_values = self(self.input_tensor)[:,0:1]
                
                    predicted_values = predicted_values*aa
                    supg_tau = self(self.input_tensor)[:,1:2]
                    supg_tau = tau*tf.sigmoid(supg_tau) * bb

                # compute the gradients of the predicted values wrt the input which is (x, y)
                gradients = tape1.gradient(predicted_values, self.input_tensor)
            gradients_2nd = tape2.gradient(gradients, self.input_tensor)       

            # Split the gradients into x and y components and reshape them to (-1, 1)
            # the reshaping is done for the tensorial operations purposes (refer Notebook)
            pred_grad_x = tf.reshape(gradients[:, 0], [self.n_cells, self.pre_multiplier_grad_x.shape[-1]])  # shape : (N_cells , N_quadrature_points)
            pred_grad_y = tf.reshape(gradients[:, 1], [self.n_cells, self.pre_multiplier_grad_y.shape[-1]])  # shape : (N_cells , N_quadrature_points)
            supg_tau = tf.reshape(supg_tau, [self.n_cells, self.pre_multiplier_grad_y.shape[-1]])
            pred_val = tf.reshape(predicted_values, [self.n_cells, self.pre_multiplier_val.shape[-1]])  # shape : (N_cells , N_quadrature_points)

            pred_grad_grad_x = tf.reshape(gradients_2nd[:, 0], [self.n_cells, self.pre_multiplier_grad_x.shape[-1]])  # shape : (N_cells , N_quadrature_points)
            pred_grad_grad_y = tf.reshape(gradients_2nd[:, 1], [self.n_cells, self.pre_multiplier_grad_y.shape[-1]])  # shape : (N_cells , N_quadrature_points)
            cells_residual, pde_residual, supg_residual = self.loss_function(test_shape_val_mat = self.pre_multiplier_val, test_grad_x_mat = self.pre_multiplier_grad_x, \
                                test_grad_y_mat = self.pre_multiplier_grad_y, pred_nn = pred_val, \
                                pred_grad_x_nn = pred_grad_x, pred_grad_y_nn = pred_grad_y ,\
                                pred_grad_grad_x_nn = pred_grad_grad_x, pred_grad_grad_y_nn = pred_grad_grad_y, \
                                forcing_function = self.force_matrix, real_forcing_function = self.real_forcing_function, \
                                bilinear_params = bilinear_params_dict, supg_tau = supg_tau)


            residual = tf.reduce_sum(cells_residual) 
            pde_residual = tf.reduce_sum(pde_residual) 
            supg_residual = tf.reduce_sum(supg_residual) 


            # Compute the total loss for the PDE
            total_pde_loss = pd_lambda*(total_pde_loss + residual)


            # print shapes of the predicted values and the actual values
            # boundary_loss = tf.reduce_mean(tf.square(predicted_values_dirichlet - self.dirichlet_actual), axis=0)

            # tf.print("Boundary Loss : ", boundary_loss)
            # tf.print("Boundary Loss Shape : ", boundary_loss.shape)
            # tf.print("Total PDE Loss : ", total_pde_loss)
            # tf.print("Total PDE Loss Shape : ", total_pde_loss.shape)

            # Compute Total Loss with L2 Regularization (only for the weights and not the biases)
            l2_regularisation = l2_lambda * tf.reduce_sum([tf.nn.l2_loss(var) for var in self.trainable_variables if 'kernel' in var.name])
            total_loss = total_pde_loss + l2_regularisation


        trainable_vars = self.trainable_variables
        self.gradients = tape.gradient(total_loss, trainable_vars)
        self.optimizer.apply_gradients(zip(self.gradients, trainable_vars))

        # backward compatibility, adding the loss values to a dictionary
        boundary_loss = 0.0
        
        return {"loss_pde": total_pde_loss, "loss_dirichlet": boundary_loss, "loss": total_loss, "cells_residual": cells_residual, \
                "l2_regularisation": l2_regularisation, \
                    "pde_loss(without_supg)": pde_residual, "supg_loss": supg_residual, "k" : self.k , "k1" : self.k1}
